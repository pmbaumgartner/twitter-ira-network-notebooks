{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter IRA Data Analysis\n",
    "\n",
    "On October 16, 2018, Twitter released several large datasets of tweets and account information related to foreign influence of US elections. From Twitter's [Elections Integrity](https://about.twitter.com/en_us/values/elections-integrity.html) page:\n",
    "\n",
    "> Our initial disclosures cover two previously disclosed campaigns, and include information from 3,841 accounts believed to be connected to the Russian Internet Research Agency, and 770 accounts believed to originate in Iran. For additional information about this disclosure, see our announcement.\n",
    ">\n",
    "> These datasets include all public, nondeleted Tweets and media (e.g., images and videos) from accounts we believe are connected to state-backed information operations. Tweets deleted by these users prior to their suspension (which are not included in these datasets) comprise less than 1% of their overall activity. Note that not all of the accounts we identified as connected to these campaigns actively Tweeted, so the number of accounts represented in the datasets may be less than the total number of accounts listed here.\n",
    "\n",
    "We use this archive of tweets to do a network analysis of the accounts related to the Russian Internet Research Agency (IRA). This is the larger of the two datasets they released. The tweet level information contains three different types of relationships between users: retweets, replies, and mentions. We'll use all three to build our network. The first part of this tutorial will handle the data cleaning necessary to generate a network.\n",
    "\n",
    "Large scale network analysis is then completed using [graph-tool](https://graph-tool.skewed.de/). The graph-tool library has a unique API that might seem a little tricky at first, particularly if you're coming from using something like NetworkX for network analysis. However, once the fundamentals of the API are understood, graph-tool enables powerful analysis: it's typically much faster than NetworkX and the visualizations are much better. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import\n",
    "\n",
    "A few notes:\n",
    "1. I wasn't able to load in the IRA data directly from the `.zip` file downloaded, so I had to manually extract it.\n",
    "2. The dataset is fairly large, so we're only going to use the columns we actually need to store less data in memory\n",
    "3. The `user_mentions` column is a list of other `userid`s, but it gets imported as a string. Thus, we'll have to convert this column from a string representation of a list to an actual list object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_list_split(input_str):\n",
    "    \"\"\"Converts a list as a string to a list object\"\"\"\n",
    "    if not isinstance(input_str, str):\n",
    "        return np.nan\n",
    "    items_str = str(input_str)[1:-1] # first and last characters are \"[\" and \"]\"\n",
    "    splits = [str(item) for item in items_str.split(\", \") if str(item)]\n",
    "    if splits:\n",
    "        return splits\n",
    "    else: \n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = {\"tweetid\" : str,\n",
    " \"userid\" : str,\n",
    " \"in_reply_to_userid\" : str, \n",
    " \"retweet_userid\" : str}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (pd.read_csv(\"./data/ira_tweets_csv_hashed.csv\",\n",
    "                 dtype=dtype,\n",
    "                 usecols=[\"tweetid\", \"userid\", \"in_reply_to_userid\", \"retweet_userid\", \"user_mentions\"])\n",
    "      .assign(user_mentions=lambda x: x['user_mentions'].apply(str_list_split)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9041308 entries, 0 to 9041307\n",
      "Data columns (total 5 columns):\n",
      "tweetid               object\n",
      "userid                object\n",
      "in_reply_to_userid    object\n",
      "retweet_userid        object\n",
      "user_mentions         object\n",
      "dtypes: object(5)\n",
      "memory usage: 344.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetid</th>\n",
       "      <th>userid</th>\n",
       "      <th>in_reply_to_userid</th>\n",
       "      <th>retweet_userid</th>\n",
       "      <th>user_mentions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>877919995476496385</td>\n",
       "      <td>249064136b1c5cb00a705316ab73dd9b53785748ab757f...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2572896396</td>\n",
       "      <td>[2572896396]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>492388766930444288</td>\n",
       "      <td>0974d5dbee4ca9bd6c3b46d62a5cbdbd5c0d86e196b624...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>719455077589721089</td>\n",
       "      <td>bda40f262856eee77c48a332e5eb23bc4f1943d600867d...</td>\n",
       "      <td>40807205</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[40807205]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>536179342423105537</td>\n",
       "      <td>bda40f262856eee77c48a332e5eb23bc4f1943d600867d...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>841410788409630720</td>\n",
       "      <td>a53ed619f1dea6015c7c878bf744b0eefe8f7272dccf34...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweetid                                             userid  \\\n",
       "0  877919995476496385  249064136b1c5cb00a705316ab73dd9b53785748ab757f...   \n",
       "1  492388766930444288  0974d5dbee4ca9bd6c3b46d62a5cbdbd5c0d86e196b624...   \n",
       "2  719455077589721089  bda40f262856eee77c48a332e5eb23bc4f1943d600867d...   \n",
       "3  536179342423105537  bda40f262856eee77c48a332e5eb23bc4f1943d600867d...   \n",
       "4  841410788409630720  a53ed619f1dea6015c7c878bf744b0eefe8f7272dccf34...   \n",
       "\n",
       "  in_reply_to_userid retweet_userid user_mentions  \n",
       "0                NaN     2572896396  [2572896396]  \n",
       "1                NaN            NaN           NaN  \n",
       "2           40807205            NaN    [40807205]  \n",
       "3                NaN            NaN           NaN  \n",
       "4                NaN            NaN           NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edge Weight Creation\n",
    "\n",
    "We'll process the data in a way to support network analysis. First, we want to find the unique users in the dataset. We need these unique users to operate on the retweet, reply, and mention columns so that we're only capturing relationships between users in our dataset -- that is, if a user in our dataset retweeted another userid that isn't in our dataset, then we won't use that information. This is beacuse we want to include only bidirectional information, but we don't know if `userid`s outside of our dataset are retweeting `userid`s inside our dataset.\n",
    "\n",
    "This is fairly straightforward for the retweet and reply relationships, since a user can only retweet or reply to another single user at a time. The mentions column requires a bit more work, since multiple users can be mentioned within a tweet. For mentions, we'll create a new variable that only has mentions of other users within our dataset. Then, we'll iterate through each tweet and create a new relationship for each mention included in the tweet.\n",
    "\n",
    "For all relationships we'll exclude self-retweets, self-replies, and self-mentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_users = set(df[\"userid\"].drop_duplicates())\n",
    "\n",
    "rt_df = df[\n",
    "    (df[\"retweet_userid\"].isin(unique_users)) & (df[\"userid\"] != df[\"retweet_userid\"])\n",
    "]\n",
    "\n",
    "reply_df = df[\n",
    "    (df[\"in_reply_to_userid\"].isin(unique_users)) & (df[\"userid\"] != df[\"in_reply_to_userid\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_mentions_in_dataset(mentions):\n",
    "    return [m for m in mentions if m in unique_users]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"mentions_in_dataset\"] = df[\"user_mentions\"].dropna().apply(lambda x: keep_mentions_in_dataset(x)).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mention_edges = []\n",
    "for row in df.dropna(subset=['mentions_in_dataset']).itertuples():\n",
    "    userid = row.userid\n",
    "    for user_mention in row.mentions_in_dataset:\n",
    "        if userid != user_mention:\n",
    "            mention_data = (userid, user_mention)\n",
    "            mention_edges.append(mention_data)\n",
    "            \n",
    "mention_df = pd.DataFrame(mention_edges, columns=['userid', 'user_mention'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Great Weight Merge\n",
    "Now that we have our list of relationships, we're going to aggregate them by counting the number of directed relationship between each pair of users. We'll calculate this for each type of relationship, and then merge all the datasets together.\n",
    "\n",
    "We'll standardize the relationships by calling the originating `userid` column `source`, and the receiving column `target`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_weights = (\n",
    "    rt_df.groupby([\"userid\", \"retweet_userid\"])\n",
    "    .size()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"userid\": \"source\", \"retweet_userid\": \"target\", 0: \"rt_weight\"})\n",
    ")\n",
    "reply_weights = (\n",
    "    reply_df.groupby([\"userid\", \"in_reply_to_userid\"])\n",
    "    .size()\n",
    "    .reset_index()\n",
    "    .rename(\n",
    "        columns={\"userid\": \"source\", \"in_reply_to_userid\": \"target\", 0: \"reply_weight\"}\n",
    "    )\n",
    ")\n",
    "mention_weights = (\n",
    "    mention_df.groupby([\"userid\", \"user_mention\"])\n",
    "    .size()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"userid\": \"source\", \"user_mention\": \"target\", 0: \"mention_weight\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N Retweets: 168074\n",
      "N Replies: 20276\n",
      "N Mentions: 180345\n"
     ]
    }
   ],
   "source": [
    "print(\"N Retweets:\", len(rt_weights))\n",
    "print(\"N Replies:\", len(reply_weights))\n",
    "print(\"N Mentions:\", len(mention_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_weights = (\n",
    "    rt_weights\n",
    "    .merge(reply_weights, on=[\"source\", \"target\"], how=\"outer\")\n",
    "    .merge(mention_weights, on=[\"source\", \"target\"], how=\"outer\")\n",
    "    .fillna(0)\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rt_weight</th>\n",
       "      <th>reply_weight</th>\n",
       "      <th>mention_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>181015.000000</td>\n",
       "      <td>181015.000000</td>\n",
       "      <td>181015.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.945502</td>\n",
       "      <td>1.012601</td>\n",
       "      <td>5.902864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>15.569890</td>\n",
       "      <td>19.088364</td>\n",
       "      <td>22.901294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1624.000000</td>\n",
       "      <td>2011.000000</td>\n",
       "      <td>1683.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           rt_weight   reply_weight  mention_weight\n",
       "count  181015.000000  181015.000000   181015.000000\n",
       "mean        4.945502       1.012601        5.902864\n",
       "std        15.569890      19.088364       22.901294\n",
       "min         0.000000       0.000000        0.000000\n",
       "25%         1.000000       0.000000        1.000000\n",
       "50%         1.000000       0.000000        2.000000\n",
       "75%         4.000000       0.000000        5.000000\n",
       "max      1624.000000    2011.000000     1683.000000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_weights.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Node List for `graph-tool`\n",
    "\n",
    "Now that we have a list of the relationships, we're going to build a list of the unique users in our dataset.  In a network analysis context, these are called our \"nodes\" or \"vertices\". Because of how the `graph-tool` API operates, we're going to index all of of our nodes to make it easy to create the graph.\n",
    "\n",
    "The process goes like this:\n",
    "1. Find all the unique nodes by taking the `set` of both the `source` and `target` columns\n",
    "2. Sort this set of unique nodes by name. This isn't necessary, but since sets are unordered, its nice to put them in a sorted list first so we can depend on the indexing being deterministic. \n",
    "3. Index the nodes and create maps for `index -> node` and `node -> index`\n",
    "4. Apply the node indexing back to the edges data.\n",
    "\n",
    "Creating the indexed list of nodes is the essential step here for use with `graph-tool`. All of the relationships and properties of the nodes tie back to their index, so it's important to keep the index map around for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nodes = set(all_weights['source'].drop_duplicates()) | set(all_weights['target'].drop_duplicates())\n",
    "all_nodes = sorted(list(all_nodes))\n",
    "node_map = dict(enumerate(all_nodes))\n",
    "node_map_inv = {v : k for k, v in node_map.items()}\n",
    "\n",
    "all_weights['source_id'] = all_weights['source'].map(node_map_inv)\n",
    "all_weights['target_id'] = all_weights['target'].map(node_map_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export\n",
    "\n",
    "Finally, we'll export our node and edges list to a `.csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = pd.DataFrame(list(node_map.items()), columns=['index', 'userid'])\n",
    "nodes.to_csv('./data/nodes.csv')\n",
    "all_weights.to_csv('./data/edge_weights.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
